---
title: "Practical machine learning course project"
author: "Benny Zuse Rousso"
date: "7 August 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## I. Introduction

This document is the final project of the course *Practical Machine Learning* from the *Specialization in Data Science* offered by John Hopkins University through Coursera MOOC platform. The goal of this project is to predict how well barbell lifts were performed in a trial with 6 participants (expressed by the variable "classe" in the data). Each participant had accelorometers from devices such as *Jawbone Up*, *Nike FuelBand* and *Fitbit* on the belt, forearm, arm and dumbells. 
Data was made available by Velloso et al. (2013) http://groupware.les.inf.puc-rio.br/har


## 2. Material and methods

Particularly for this project, the data used for training and testing the models were defined as the ones available in the links below, respectively:

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Testing data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

This project encompass 3 phases: 
*i)*loading and pre-processing the data;
*ii)*training, testing and validation of fitted models, and;
*iii)*decision of selected model and application in final course quiz.

Data was analyzed using Rstudio and the below packages

```{r libraries loading}
library(caret)
library(rpart)
```

## Phase 1) loading and pre-processing the data
```{r loading data}
urlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(urlTrain))
training$classe <- as.factor(training$classe)
testing <- read.csv(url(urlTest))
```

Since the testing dataset is small (20 obs.) and the training set is large (19622 obs.), I performed a cross-validation using Random subsampling since it was assumed that observations were not time dependent (i.e not need to divide the training set in continuous time chunks as done by k-fold cross-validation). Random sampling was defined in the proportion 70-30% for training and validation sets. For reproducibility, I set a specific seed (69).

``` {r data partitioning}
set.seed(69)
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
training1 <-training[inTrain,]
validation <-training[-inTrain,]
```

The number of variables is large (160), thus I explored for near-zero covariates to reduce model dimensionality. 

```{r nzv}
nzv <- nearZeroVar(training1,saveMetrics=TRUE)
nzv_names <- rownames(nzv[nzv$nzv==TRUE,])
training1 <- training1[,!names(training)%in% nzv_names]
```

56 variables were identified as near-zero covariates, resulting in 104 variables remaining. Since this is still a large number of possible predictors, I explored more the data and noticed that many have lots of NA.Variables that had more than 95% of NA were removed.

```{r NA removal}
NA_vars <- sapply(training1, function(x) mean(is.na(x)))>0.95
training1 <- training1[,NA_vars==FALSE]
```

45 variables satisfied the previous criteria, leaving further 59 variables. A PCA could have been further employed to further reduce the model dimensionality, but I preferred to maitain these variables and test the accuracy of developped models before reducing further the features of the model. In addition, I only removed the ID variables (1 to 5).

```{r ID vars removal}
# Remove ID vars (1:5)
training1<-training1[,-c(1:5)]

```

## Phase 2) Training and cross-validating he models
I have established a minimum accuracy of 0.99 as goal for my prediction model. I developed a random forest (rf), a decision tree with CART (rpart), and a Generalized boosted model (gbm) to see if I could achieve this fit

```{r  models training}
#Random Forest
control_rf <- trainControl(method="cv", number=3, verboseIter = FALSE)
model_rf <- train(classe~.,data=training1,method="rf", trControl=control_rf)
model_rf$finalModel

#Decision Tree
model_dt <- rpart(classe~., data=training1, method="class")

#Generalized boosted model
control_gbm <- trainControl(method = "repeatedcv", number=5, repeats=1)
model_gbm <-train(classe~., data=training1, method="gbm",trControl=control_gbm, verbose=FALSE)
model_gbm$finalModel
```

After training the models, I predicted the new 'classe' values using the validation dataset
```{r models validation}
#Random Forest
predict_rf <- predict(model_rf, newdata=validation)

#Decision Tree
predict_dt <- predict(model_dt, newdata=validation, type = "class")

#Generalized boosted model
predict_gbm <- predict(model_gbm, newdata=validation)
```

After which I assessed the accuracy against the real values of the validation dataset
```{r confusion matrix}
cm_rf <-confusionMatrix(predict_rf,validation$classe)
cm_dt <-confusionMatrix(predict_dt,validation$classe)
cm_gbm <-confusionMatrix(predict_gbm,validation$classe)

accuracy_rf <-cm_rf$overall['Accuracy']
accuracy_dt <-cm_dt$overall['Accuracy']
accuracy_gbm<-cm_gbm$overall['Accuracy']

cbind(accuracy_rf,accuracy_dt,accuracy_gbm)
```

Based on the results from each prediction model, Random Forest achieved the defined threshold of Accuracy >0.99 and was thus the selected to be applied in the third phase of the project. As main drawback of this method was the computational time, which was signficantly larger than the others. This should be taken into account if very large datasets are supposed to be assessed - an alternative to keep this model would be to further reduce model dimensionality (e.g. PCA), but this may also affect performance. GBM showed a very good performance too with a significant faster computation time.

# 3) Testing the selected model against testing data

Applying the chosen model (Random Forest) against the testing data, the predictions of classe are:
```{r testing}
predict_testing <-predict(model_rf,newdata = testing)
predict_testing
```